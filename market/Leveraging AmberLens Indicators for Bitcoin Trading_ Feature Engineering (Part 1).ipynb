{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0393c9ab-2bd4-4332-8025-7e86590762d3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "With the launch of AmberLens, analysts and researchers can now visualize and derive significant insights into various facets of the digital asset world. The platform offers a range of dashboards that highlight key aspects of different domains in the space as well as topics that are top of mind for the industry. While these metrics are highly valuable for understanding macro trends, leveraging these insights for more granular, short-term trading is a non-trivial task.\n",
    "\n",
    "In this three part series, accompanied with a Databricks notebook providing all the code used, we will elucidate how to utilize AmberLens to the fullest. Traders and analysts will be enabled to dive deeper and discover new insights as the series outlines how to create a Bitcoin trading strategy that leverages AmberLens’ technical indicators.\n",
    "\n",
    "In Part 1 of this series, we will do some basic feature engineering to create new metrics and evaluate their usefulness. In Part 2, we will use these features to train and test machine learning models. Finally, in Part 3, we will discuss how to use these features and machine learning models to create a trading strategy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1863251f-ef68-44cc-ae40-797bec0be403",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Reading in the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cd60291-17ba-45c9-9ab0-50de6e663810",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here is a list of the Bitcoin metrics we have available to use: \n",
    "\n",
    "* Net Unrealized Profit / Loss (NUPL)\n",
    "* MVRV Z-Score\n",
    "* Percent of Addresses / Supply in Profit\n",
    "* Reserve Risk\n",
    "* Stock-To-Flow\n",
    "* Puell Multiple\n",
    "* Miner Supply Spent vs. Sold\n",
    "* Hodl Net Position Change\n",
    "* Bitcoin Yardstick\n",
    "* Monthly Active Addresses\n",
    "* Monthly New Addresses / Momentum\n",
    "* Liquid vs. Illiquid Supply\n",
    "* Address Balances Buckets (BTC and USD)\n",
    "* HODL Waves\n",
    "* Miner Position Index\n",
    "\n",
    "With the launch of AmberLens, analysts and researchers can now visualize and derive significant insights into various facets of the digital asset world. The platform offers a range of dashboards that highlight key aspects of different domains in the space as well as topics that are top of mind for the industry. While these metrics are highly valuable for understanding macro trends, leveraging these insights for more granular, short-term trading is a non-trivial task.\n",
    "\n",
    "In this three part series, accompanied with a Databricks notebook providing all the code used, we will elucidate how to utilize AmberLens to the fullest. Traders and analysts will be enabled to dive deeper and discover new insights as the series outlines how to create a Bitcoin trading strategy that leverages AmberLens’ technical indicators.\n",
    "\n",
    "In Part 1 of this series, we will do some basic feature engineering to create new metrics and evaluate their usefulness. In Part 2, we will use these features to train and test machine learning models. Finally, in Part 3, we will discuss how to use these features and machine learning models to create a trading strategy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21930482-40ae-4927-af79-f94754f8fdd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS <tablename>\n",
    "USING snowflake\n",
    "OPTIONS (\n",
    "    host        '<hostname>.snowflakecomputing.com',\n",
    "    port        '443',\n",
    "    user        '<username>',\n",
    "    password    '<password>',\n",
    "    sfWarehouse '<warehouse>',\n",
    "    sfRole      'DATABRICKS',\n",
    "    database    'AMBERDATA_LOADING',\n",
    "    schema      'RAW_MARKET',\n",
    "    dbtable     'SILVER_SPOT_PRICE_BTC_USD_DAILY'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e161f6e0-dc08-42e7-8079-dfc65c19fca5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Then we can query these tables like normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2118604c-73aa-49c5-8b8a-47343050bd29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "price_df = spark.sql(\"select timestamp as date, price from analytics.silver_spot_price_btc_usd_daily\")\n",
    "nupl_df = spark.sql(\"select date, nupl from analytics.gold_btc_nupl_daily\")\n",
    "mvrv_df = spark.sql(\"select date, mvrvZ from analytics.gold_btc_mvrv_daily\")\n",
    "sip_df = spark.sql(\"select date, supplyInProfitPercentage from analytics.gold_btc_supply_in_profit_daily\")\n",
    "rr_df = spark.sql(\"select date, reserveRisk from analytics.gold_btc_reserve_risk_daily\")\n",
    "stf_df = spark.sql(\"select date, ratio as stockToFlow from analytics.gold_btc_stock_to_flow_daily\")\n",
    "pm_df = spark.sql(\"select date, puellMultiple from analytics.gold_btc_puell_multiple_daily\")\n",
    "hnpc_df = spark.sql(\"select date, hodlNetPositionChangeDaily as hodlNetPositionChange from analytics.gold_btc_hodl_net_position_change_daily\")\n",
    "by_df = spark.sql(\"select date, yardstick from analytics.gold_btc_yardstick_daily\")\n",
    "am_df = spark.sql(\"select date, passive_addresses, active_addresses, new_addresses, new_inputs, new_outputs, 30_day_new_address_ma, 365_day_new_address_ma from analytics.gold_btc_address_momentum_daily\")\n",
    "hw_df = spark.sql(\"\"\"\n",
    "    with max_rundate as (select max(rundate) as rundate from analytics.btc_hodl_wave)\n",
    "    select \n",
    "        date, \n",
    "        utxo_value_under_1d, utxo_value_1d_1w, utxo_value_1w_1m, utxo_value_1m_3m,\n",
    "        utxo_value_3m_6m, utxo_value_6m_12m, utxo_value_12m_18m, utxo_value_18m_24m, \n",
    "        utxo_value_2y_3y, utxo_value_3y_5y, utxo_value_5y_8y, utxo_value_greater_8y, \n",
    "        utxo_count_under_1d, utxo_count_1d_1w, utxo_count_1w_1m, \n",
    "        utxo_count_1m_3m, utxo_count_3m_6m, utxo_count_6m_12m, utxo_count_12m_18m, utxo_count_18m_24m, \n",
    "        utxo_count_2y_3y, utxo_count_3y_5y, utxo_count_5y_8y, utxo_count_greater_8y\n",
    "    from analytics.btc_hodl_wave where rundate = (select rundate from max_rundate)\"\"\")\n",
    "mpi_df = spark.sql(\"select date, minerPositionIndex from analytics.gold_btc_miner_position_index_daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab9653e8-7d22-4aef-846c-c93f96d91aed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For liquid/illiquid supply and address buckets, we will need to do some additional work to read them in as features. Because they are pivoted tables, we will need to \"unpivot\" them in order to get them in the same shape as the other datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd016255-9a49-41ee-ac7a-45823b667bbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, max\n",
    "\n",
    "lis_df = (spark.sql(\"select date, liqudity_rank, total_balance from analytics.gold_btc_liquid_illiquid_supply\")\n",
    "    .groupBy(\"date\").pivot(\"liqudity_rank\").agg(max(\"total_balance\"))\n",
    "    )\n",
    "display(lis_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd90e812-7117-44c8-b049-17f1305d98e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "And join them into one df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "461d973f-9e23-45b1-b506-43e68ec3e662",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (price_df\n",
    "      .join(nupl_df, on=[\"date\"])\n",
    "      .join(mvrv_df, on=[\"date\"])\n",
    "      .join(sip_df, on=[\"date\"])\n",
    "      .join(rr_df, on=[\"date\"])\n",
    "      .join(stf_df, on=[\"date\"])\n",
    "      .join(pm_df, on=[\"date\"])\n",
    "      .join(hnpc_df, on=[\"date\"])\n",
    "      .join(by_df, on=[\"date\"])\n",
    "      .join(am_df, on=[\"date\"])\n",
    "      .join(lis_df, on=[\"date\"])\n",
    "      .join(mpi_df, on=[\"date\"])\n",
    "      .filter(\"date > '2015-01-01'\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baf3e259-052c-4335-95b7-7c5b49a40dde",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "Now that we have the data, let's explore how we can visualize this data and draw insights from it. In Databricks, the `display` function provides very functional initial EDA tools. After we `display(df)` we can then use the data profiler to see basic summary statistics for each column, as well as the overall distribution of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "372b20e8-30aa-49d5-aa63-91af28127c2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79b2e52a-69e9-4d06-ad12-27195bf91425",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Data profiles are a great way to get a first look at the data. Of course, since this data is from AmberLens, it is relatively clean and complete. We can see that the minimum date is 2015 and the maximum date is April 29, 2024 (this snapshot was taken on April 30, 2024). Data skew is present in several indicators, both left and right depending on the metric. Data skew implies we may need some sort of normalization technique in order to use them correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cab9b5ae-320e-4e03-8fcc-8e23f8601562",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1faeaea6-51cb-4e7b-b1dd-e16c47a7e21a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Moving Averages\n",
    "\n",
    "Moving averages are a fundamental technique in feature engineering, particularly valuable for time series data. They serve to smooth out short-term fluctuations and highlight longer-term trends or cycles in the data, making it easier to identify underlying patterns. This smoothing effect is crucial because raw data can often be noisy, which can obscure the true signal that models need to learn from. By averaging data points over a specified period, moving averages reduce the impact of random variability, leading to more robust features for predictive modeling.\n",
    "\n",
    "The intuition behind using moving averages lies in their ability to provide a clearer view of the data's direction over time. For instance, in financial markets, moving averages can help distinguish between random price movements and significant trends, aiding in better forecasting and decision-making. They effectively filter out the 'noise,' allowing the model to focus on the 'signal,' which is the true underlying trend. This makes moving averages particularly useful for applications like stock price prediction, weather forecasting, and demand planning, where understanding the trend over time is crucial.\n",
    "\n",
    "For example, let's take a look at supply in profit. Suppose we have a belief that the trend or direction of supply in profit is more important than the percent of supply in profit on a given day. Particularly, we believe that the overall trend of the graph gives us signals as to whether we have hit the top or bottom of a market cycle. To smooth short term fluctuations, let's create a 7 day, 30 day, and 365 day moving average for this time series, and then plot it with price to observe the relationship between the two variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f81302a-e629-490d-8d0e-e8c9cfad445e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "w = Window().orderBy(\"date\")\n",
    "\n",
    "display(df\n",
    "    .select(\"date\", \"price\", \"supplyInProfitPercentage\")\n",
    "    .withColumn(\"SIP_MA7Days\", avg(\"supplyInProfitPercentage\").over(w.rowsBetween(-7, 0)))\n",
    "    .withColumn(\"SIP_MA30Days\", avg(\"supplyInProfitPercentage\").over(w.rowsBetween(-30, 0)))\n",
    "    .withColumn(\"SIP_MA365Days\", avg(\"supplyInProfitPercentage\").over(w.rowsBetween(-365, 0)))\n",
    "\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5241de05-d276-445d-8d32-039b9fa6a1ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Of course, there is a relationship between supply in profit and price: as price goes up, supply in profit will increase, and vice versa but what we are interested in is how supply in profit conveys market tops or bottoms.\n",
    "\n",
    "Raw supply in profit is very noisy; by smoothing out the curve we can notice some interesting trends. For example, in the 30-day MA for supply in profit, when it clears higher than 80%, it is almost always a maximum for the curve. This is a great top indicator as it's a clear signal for participants to claim their gains before price correction. This logic also applies as a market bottom indicator: the 30-day smoothed supply in profit percentage rarely drops below 30%. This signal can act as a clear buy signal with the market likely bottoming out; we will likely see prices start to rise again. Using a simple moving average transformation, we were able to create a new metric with clear entry and exit strategies. In Part 2, we will explore how we can devise a trading strategy using these types of signals and compare them against other baseline models, and in Part 3,  we’ll explore how we can use Machine Learning to assist in feature engineering.\n",
    "\n",
    "Continuing this example, the 7-day and 365-day smoothed supply in profit curves are less useful, since they do not provide as much information. For example, in the 7-day smoothed curve, we see it cross the 80% and 30% threshold multiple times in one market cycle, indicating that it is only slightly less noisy than the current supply in profit. The 365-day smoothed curve is too smooth, and we would only make four trades using the previous 30-day heuristic we came up with. This is not necessarily a bad thing: we can use the 365-day curve as a measure of confidence that we are in a true market top or bottom. That is, if the 365-day curve reaches either extreme we have more confidence in the market truly bottoming or topping out. This shows a few ways in which we can leverage different moving average windows to capture directionality and trends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8da57416-0f18-4eda-ab71-4978ada0ec4e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Daily / Monthly Net Change\n",
    "\n",
    "The periodic net change in a time series, defined as the difference between a set period of data points, is a powerful feature for capturing short-term dynamics and volatility within the data. The most common expression of this metric is at a daily or monthly granularity, aptly named daily net change or monthly net change. This feature engineering technique gives us insight into the immediate momentum and direction of change in a time series, providing insights into how the value of a variable evolves periodically. This exact concept is one of our metrics, HODL Net Position Change. The base time series, HODLed coins, is a useful metric by itself, but by taking the daily or monthly net change, the engineered metric gives us better insight into the magnitude at which Bitcoin users are HODLing or not HODLing their coins.\n",
    "\n",
    "By incorporating a periodic net change as a feature, models can gain a nuanced understanding of temporal dependencies and short-term fluctuations that might be critical for accurate predictions. This feature helps in identifying patterns such as trends, reversals, and volatility spikes, which might not be evident from the raw data alone. Additionally, it can enhance the model's ability to detect anomalies, such as sudden drops or surges, which are important for risk management and decision-making processes. Overall, the periodic net change enriches the feature set, enabling more responsive and insightful modeling of time series data.\n",
    "\n",
    "Let's look at the periodic net change for another one of our series. Suppose we have a theory that the daily change in a HODL wave gives some insight into whether the price is dropping or rising. The idea here is that as long-term bands gain or lose coins, they convey information about the state of the network (if fewer people are HODLing, then we will see a downward price pressure, and vice versa). Let's first group different waves into three different bands to make analysis easier: short-term HODLers (< 3 months), mid-term HODLers (3 months - 3 years) and long-term HODLers (1 year+).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afc2c3ef-5d0d-4b8c-8340-ced518d0609e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(hw_df.orderBy(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd9c56b9-9924-4587-aa91-abb196f2c143",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "behavior_df = (hw_df\n",
    "    .withColumn(\"short_term_hodlers\", col(\"utxo_value_under_1d\") + col(\"utxo_value_1d_1w\") + col(\"utxo_value_1w_1m\") + col(\"utxo_value_1m_3m\"))\n",
    "    .withColumn(\"mid_term_hodlers\", col(\"utxo_value_3m_6m\") + col(\"utxo_value_6m_12m\") + col(\"utxo_value_12m_18m\") + col(\"utxo_value_18m_24m\"))\n",
    "    .withColumn(\"long_term_hodlers\", col(\"utxo_value_2y_3y\") + col(\"utxo_value_3y_5y\") + col(\"utxo_value_5y_8y\") + col(\"utxo_value_greater_8y\"))\n",
    "    .select(\"date\", \"short_term_hodlers\", \"mid_term_hodlers\", \"long_term_hodlers\") \n",
    "    .join(price_df, on=[\"date\"])       \n",
    ")\n",
    "\n",
    "display(behavior_df.orderBy(col(\"date\").desc()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a6a3397-3c0c-4f37-9a04-53454449c91d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Some initial observations after grouping: short-term holders experience a lot more variance in their wave value, while mid to long-term HODLers experience relatively longer sinusoidal waves with lower variation in their magnitude. This might lead us to believe that short-term HODL waves can give us some insight into price or price volatility. Volatility is typically measured as the standard deviation of log returns multiplied by the square root of periods (in this case, 365 since crypto trades year-round). Coincidentally, to model volatility we will also need to take the daily difference of price to get daily return, which showcases another use case for daily net change. We choose to look at price volatility because some trade strategies or machine learning models want to model volatility as entry or exit signals, and so building a feature to model volatility can be very useful.\n",
    "\n",
    "Volatility is typically measured as the standard deviation of log returns multiplied by the square root of periods (in this case, 365 days). To model volatility we will also need to take the daily difference of price to get daily return, which showcases another use case for daily net change. Let's model to see if we can capture this relationship.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eaf83f3-aa2a-4ccb-abbc-0a18b43cbefe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, log, stddev, sqrt, lit\n",
    "\n",
    "w = Window.orderBy(\"date\")\n",
    "\n",
    "display(\n",
    "    behavior_df\n",
    "    .withColumn(\"short_monthly_net_position_change\", col(\"short_term_hodlers\") - lag(\"short_term_hodlers\", 30).over(w))\n",
    "    .withColumn(\"mid_monthly_net_position_change\", col(\"mid_term_hodlers\") - lag(\"mid_term_hodlers\", 30).over(w))\n",
    "    .withColumn(\"long_monthly_net_position_change\", col(\"long_term_hodlers\") - lag(\"long_term_hodlers\", 30).over(w))\n",
    "    .withColumn(\"price_returns_log\", log(col(\"price\") / lag(\"price\", 1).over(w)))\n",
    "    .withColumn(\"price_volatility_rolling\", stddev(\"price_returns_log\").over(Window().orderBy(\"date\").rowsBetween(-30, 0)) * sqrt(lit(365) / lit(30))) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7e7c6c8-8b51-41d7-a766-c2a006c8a2b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Let's break down each step of code here:\n",
    "\n",
    "```\n",
    "    .withColumn(\"short_monthly_net_position_change\", col(\"short_term_hodlers\") - lag(\"short_term_hodlers\", 30).over(w))\n",
    "    .withColumn(\"mid_monthly_net_position_change\", col(\"mid_term_hodlers\") - lag(\"mid_term_hodlers\", 30).over(w))\n",
    "    .withColumn(\"long_monthly_net_position_change\", col(\"long_term_hodlers\") - lag(\"long_term_hodlers\", 30).over(w))\n",
    "```\n",
    "\n",
    "Since HODL waves are updated monthly, we want to look at monthly net position change. This line gives us a rough approximation of monthly net position change. Months vary in length from 29 - 31 days, and we're more interested in general trends than being exact with our monthly calculation. When productionalizing this code, we can be more rigorous with ensuring we are looking at the exact net difference at the start of each month, but an approximate version will work well enough for now, since we're still doing exploratory data analysis. These three lines are simply taking each of our aggregated HODL waves, and then subtracting from the value 30 days ago. \n",
    "\n",
    "The next two lines show how to calculate price volatility\n",
    "\n",
    "```\n",
    "    .withColumn(\"price_returns_log\", log(col(\"price\") / lag(\"price\", 1).over(w)))\n",
    "    .withColumn(\"price_volatility_rolling\", stddev(\"price_returns_log\").over(Window().orderBy(\"date\").rowsBetween(-30, 0)) * sqrt(lit(365) / lit(30))) \n",
    "```\n",
    "\n",
    "Because Bitcoin is such a volatile stock, we want to look at the 30 day rolling volaility, because Bitcoin goes through short market cycles with strong volatility changes. Volatility is typically measured via log returns, and log returns is the log(price - prev_price). Using log laws, we can simplify this to log(price / prev_price), which is what the first line of code is showing. Then, we take a 30-day rolling standard deviation of log returns, and multiply by the annualization factor. Typically, this should be `sqrt(# of trading days) / (rolling window))` where the numerator is usually 252. However, since Bitcoin trades every day, we have an annualization factor of 365 / 30. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fefc8050-96c7-4b0e-8293-63ae092c36af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "Now that we have our net position change and price volatility, we can look at the relationship between the two variables. In the short-term wave, we see a very slight positive relationship between the monthly wave position change and price volatility. This confirms our initial hypothesis, which was that as the short-term wave gets larger, price volatility increases. The mid-term wave has a very flat, almost no relationship, and the long-term wave has a relatively strong negative correlation. This is fascinating because it shows that there is another relationship we didn’t account for the negative correlation between long-term waves position change and price volatility. This implies that as more people HODL for longer periods, price volatility decreases as well, which is logically sound.  This means we now have two features we can use as features to predict price volatility, as any correlation can be utilized as feature inputs into a price volatility model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29d1d3b6-a16a-4ec1-9c72-2ce2adc87343",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Conclusion\n",
    "\n",
    "Feature engineering is the first step in machine learning, as we need strong features to have strong predictive models. By showcasing two commonly used feature engineering techniques, we hope you can feel inspired to investigate similar theories or hypotheses using AmberLens data. In the next part, we will go over how we can use some of these features to build basic trading strategies that leverage these metrics for trading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b8403e8-2c83-4117-9509-debcc29dd811",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Leveraging AmberLens Indicators for Bitcoin Trading_ Feature Engineering (Part 1)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
